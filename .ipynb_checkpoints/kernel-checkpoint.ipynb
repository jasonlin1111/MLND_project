{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "05022d00-ace5-45ed-b4c2-cfb5512b57ee",
    "_uuid": "6081dda6cda852d4520e96e534bd1f1802d10f27"
   },
   "source": [
    "# Feature engineering for House Price modelling\n",
    "\n",
    "In this notebook, I will bring together various techniques for feature engineering to tackle a regression problem. I hope to give you a flavour of how to approach the end-to-end pipeline to build machine learning algorithms for regression.\n",
    "\n",
    "For more feature engineering techniques, check my new course [Feature Engineering for Machine Learning](https://www.udemy.com/feature-engineering-for-machine-learning/?couponCode=PROMO_KGG), which was recently launched on Udemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20c7d49c-ef93-4e9d-a50a-5114eccb76ec",
    "_uuid": "df3fb79effcd8b56444f5603146238eca0962c5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# for variable transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "# to build the models\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "# to evaluate the models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c2b552f-4091-4e51-806f-34d9f587bdcc",
    "_uuid": "34d29174c82aadc073b8ae8c1a5f3104a123243d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"../input/train.csv\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "135be510-da77-4150-9de2-8838f4e80a04",
    "_uuid": "93edc29a7b160d81c098adc265ad421d7a289983",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset for submission (the one on which our model will be evaluated by Kaggle)\n",
    "# it contains exactly the same variables, but not the target\n",
    "\n",
    "submission = pd.read_csv(\"../input/test.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1aaa3c7c-4153-4a06-91d5-74c439f843cf",
    "_uuid": "ea3fcaba54a8e71bdb4eddd172abec5efb614239"
   },
   "source": [
    "The House Price dataset  contains 80 different variables. We could potentially investigate each one of them individually, and I think that in a business scenario this would be the right way to proceed, as they are actually not that many. However, for the purpose of this notebook, I will try to automate the feature engineering pipeline, making some a priori decisions on when I will apply one feature engineering technique or the other.\n",
    "\n",
    "There are other good visualisation notebooks in Kaggle that you can check to get more familiar of how the variables look like.\n",
    "\n",
    "### Types of variables\n",
    "\n",
    "Let's go ahead and find out what types of variables there are in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5e09b6d-bd95-4066-87ed-0dca12e2075e",
    "_uuid": "d5571a526a6e3da6ce6af7c2635cf8a4ace3cfbc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "categorical = [var for var in data.columns if data[var].dtype=='O']\n",
    "print('There are {} categorical variables'.format(len(categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ff32595-5fb9-492c-ae3d-c6bbd8c4fed0",
    "_uuid": "b381489622b8918fd185b36dc807955bf50804f9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "numerical = [var for var in data.columns if data[var].dtype!='O']\n",
    "print('There are {} numerical variables'.format(len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c52b9fea-8ebc-416e-8d12-cfbbf68b148b",
    "_uuid": "0658e5ce7b1368069df73f4e649b85558d320d42"
   },
   "source": [
    "Numerical variables can be binary, continuous or discrete. A priori, it is good practice to know what each variable means, to then be able to differentiate continuous from discrete variables. In this notebook, I will assume that variables with a definite and low number of unique values are discrete.\n",
    "\n",
    "#### Find discrete variables\n",
    "\n",
    "To identify discrete variables, I will select from all the numerical ones, those that contain a finite and small number of distinct values. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e2925b0-f21b-4b91-b9ce-1b0e64f7fbea",
    "_uuid": "d48c3e8e327698c62acda992cccfddaf9f995886",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's visualise the values of the discrete variables\n",
    "discrete = []\n",
    "for var in numerical:\n",
    "    if len(data[var].unique())<20:\n",
    "        print(var, ' values: ', data[var].unique())\n",
    "        discrete.append(var)\n",
    "        \n",
    "print('There are {} discrete variables'.format(len(discrete)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd031801-a7e1-46c3-a6a4-7775701a363d",
    "_uuid": "6b290a065e374f4d4fb3e16ac4bb0a3c753b02f8"
   },
   "source": [
    "As you can see there are a number of discrete variables in the dataset,  for example BedroomAbvGr, with the values indicating the number of bedrooms in the House.\n",
    "\n",
    "### Types of problems the variables may present\n",
    "\n",
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "726076d8-02c6-4afb-b4f4-b7098439dddc",
    "_uuid": "4fde749f4142910ac3beb4bcbf0f9f60bbe22134",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's visualise the percentage of missing values\n",
    "for var in data.columns:\n",
    "    if data[var].isnull().sum()>0:\n",
    "        print(var, data[var].isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f510766-fb5e-4363-9938-a5f3001da2c5",
    "_uuid": "8a6d65ba5942e174d4453c921a1e5ae88a732aab"
   },
   "source": [
    "There are a few variables that contain missing information (NaN). Some of them contain a lot of missing values, and some of them only a few. Let's first identify those that contain a lot of NaN and then see how we can process the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c627089-f69a-48db-bca4-5d1efa6202b5",
    "_uuid": "4b6dc2875b288bd755aff5a798ec090d47f90ebf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's inspect the type of those variables with a lot of missing information\n",
    "for var in data.columns:\n",
    "    if data[var].isnull().mean()>0.80:\n",
    "        print(var, data[var].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3bc0281-410d-4fd7-bd85-df85e4360af3",
    "_uuid": "7eb2319b7a11ad0017ccb8a4e5f1e4fd3d61c0dd"
   },
   "source": [
    "The ones with high percentage of missing data are categorical variables. We will need to fill those out.\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "Let's find out now if the variables contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36c036eb-7aaf-4fd0-bf82-31809484cc52",
    "_uuid": "a3986ea8145bdabf0dc5b486395b24029ffcd360",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first we make a list of continuous variables (from the numerical ones)\n",
    "continuous = [var for var in numerical if var not in discrete and var not in ['Id', 'SalePrice']]\n",
    "continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc428fb1-064c-4916-b66e-de8f531d8e51",
    "_uuid": "05f20acb7edc8c89beb343b637268e092bd82c1e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make boxplots to visualise outliers in the continuous variables \n",
    "# and histograms to get an idea of the distribution\n",
    "\n",
    "for var in continuous:\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fig = sns.boxplot(y=data[var])\n",
    "    fig.set_title('')\n",
    "    fig.set_ylabel(var)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    fig = sns.distplot(data[var].dropna())\n",
    "    fig.set_ylabel('Number of houses')\n",
    "    fig.set_xlabel(var)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7cdcaa0d-5280-4cfa-82ea-cabdbb9cc262",
    "_uuid": "59ea0542937f2c25518add4c6c3b7ab0f516b2b3"
   },
   "source": [
    "Outliers can be visualised as the dots outside the  whiskers in the boxplots. The majority of the continuous variables seem to contain outliers. In addition, the majority of the variables are not normally distributed. If we are planning to build linear regression, we should tackle these to improve the model performance. I will transform the variables with a box cox to try and make them more \"Gaussian\" looking later on in the notebook.  I will not cover outlier removal in the notebook though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9bdefce4-9860-4a66-ad58-3c79051520a1",
    "_uuid": "c2d0b6083d1f8d3b6f69429d7dc19c33fbbfe917",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's look at the distribution of the target variable\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "fig = sns.boxplot(y=data['SalePrice'])\n",
    "fig.set_title('')\n",
    "fig.set_ylabel(var)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "fig = sns.distplot(data['SalePrice'].dropna())#.hist(bins=20)\n",
    "fig.set_ylabel('Number of passengers')\n",
    "fig.set_xlabel(var)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3dd44d2-1dfd-4ba1-b7c2-84b6ddd3fe48",
    "_uuid": "dce7755450d27cd3d6bd9561815b6fa6650a872d"
   },
   "source": [
    "The target variable is also skewed. So I will transform it as well to boost the performance of the algorithm.\n",
    "\n",
    "#### Outlies in discrete variables\n",
    "\n",
    "Let's calculate the percentage of houses for each  of the values that can take the discrete variables. I will call outliers, those values that are present in less than 1% of the houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c7db5f8-753b-4c21-b336-f55d720eadc1",
    "_uuid": "8d1afddb88ac0a55e01ef78de0bfade068974698",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outlies in discrete variables\n",
    "for var in discrete:\n",
    "    print(data[var].value_counts() / np.float(len(data)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8dbb3233-b65f-4b0f-af9d-6272eb1c0e43",
    "_uuid": "ce1093a19937411b7d594e9d3f4a5b51a8efbb2d"
   },
   "source": [
    "Most of the discrete variables show values that are shared by a tiny proportion of houses in the dataset. For linear regression, this may not be a problem, but it most likely will be for tree methods. We should take this into account to improve the performance of our trees.\n",
    "\n",
    "\n",
    "#### Number of labels: cardinality\n",
    "\n",
    "Let's now check if our categorical variables have a huge number of categories. This may be a problem for some machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2939cffa-0561-45f2-a2c1-43f8e4564dd0",
    "_uuid": "7d5b9cf1465c4221ca1e0ad338caed96e9c862bd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in categorical:\n",
    "    print(var, ' contains ', len(data[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fd5b6832-3329-4594-aaea-1d8058cb927d",
    "_uuid": "5c48f22434e2023a722675853834160488a21f90"
   },
   "source": [
    "Most of the variables, contain only a few labels. Then, we do not have to deal with high cardinality. That is good news!\n",
    "\n",
    "Variables with high cardinality may affect the performance of some machine learning models, for example trees.\n",
    "\n",
    "### Separate train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80d34ca9-1354-4102-9aa3-2910e8ddd1af",
    "_uuid": "9d92c2b26f033351f4c18ced52569c27f35cea2c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's separate into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data.SalePrice, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d69ce769-6e42-4571-a2d9-cd5e60ff278e",
    "_uuid": "93ad98f5143b33decc49514e8bc588564722645d"
   },
   "source": [
    "### Engineering missing values in numerical variables\n",
    "#### Continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e8333a5-91bd-4551-af1f-76e803781b0d",
    "_uuid": "4f29c127c2c4755fdbe8e252449ed58e3cfeaad1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print variables with missing data\n",
    "for col in continuous:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, X_train[col].isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "507b2fd2-ea07-40df-9e0b-cb84dc08aac2",
    "_uuid": "c9dc1879d23325010b48e0cc3e028384e1cd6a58"
   },
   "source": [
    "- LotFrontage and GarageYrBlt contain a relatively high percentage of missing values, therefore I will create and additional variable to indicate NA, and then I will do median imputation on the original variable.\n",
    "- CMasVnrArea contains a small percentage of missing values, thus I will just do median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6f73b863-d874-483d-b820-02b4be47f584",
    "_uuid": "6d6cada0c020d5f45eca9a12e586aa931e6de454",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add variable indicating missingness + median imputation\n",
    "for df in [X_train, X_test, submission]:\n",
    "    for var in ['LotFrontage', 'GarageYrBlt']:\n",
    "        df[var+'_NA'] = np.where(df[var].isnull(), 1, 0)\n",
    "        df[var].fillna(X_train[var].median(), inplace=True) \n",
    "\n",
    "for df in [X_train, X_test, submission]:\n",
    "    df.MasVnrArea.fillna(X_train.MasVnrArea.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "27227914-9f29-4b9c-9a5e-fa5674c8439a",
    "_uuid": "946cdaaaf762d9c1aaee5f57ebfd2061d717f386"
   },
   "source": [
    "#### Discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0cec8a8a-de2f-420b-a9a7-02704c9edb13",
    "_uuid": "f1844eba39e965763ffeaac0dc9c1a4652a672f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print variables with missing data\n",
    "for col in discrete:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, X_train[col].isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9dccc27-9c07-4864-8e3f-0e7a376c5a08",
    "_uuid": "e69787100350de6730411c70c9242e26feef381b"
   },
   "source": [
    "There are no missing data in the discrete variables. Good, then we don't have to engineer them.\n",
    "\n",
    "### Engineering Missing Data in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b6308e5e-5f53-4112-a7a4-3604a50241a0",
    "_uuid": "5c0242eef7e26bdb6d172919b973c2c88312d45e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print variables with missing data\n",
    "for col in categorical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, X_train[col].isnull().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8254d77-fd9e-4eca-a89f-b10f81de49c2",
    "_uuid": "86f5343592c7099f3927c02793ee6b37e6ddb239"
   },
   "source": [
    "I will add a 'Missing' Label to all of them. If the missing data are rare, I will handle those together with rare labels in a subsequent engineering step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b25d0d7e-d9f5-4987-8b91-93028b4f9e05",
    "_uuid": "16d859b9ee9f688c2b61564b5d34cb7ca59d53b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add label indicating 'Missing' to categorical variables\n",
    "\n",
    "for df in [X_train, X_test, submission]:\n",
    "    for var in categorical:\n",
    "        df[var].fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d681cc6f-8c72-4870-a089-bdc5d1ed5b3e",
    "_uuid": "d554d11ac9e6fb28eabe07c7e8ad5d0500603c25",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check absence of null values\n",
    "for var in X_train.columns:\n",
    "    if X_train[var].isnull().sum()>0:\n",
    "        print(var, X_train[var].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1472571a-b621-4230-8c39-62fd684fffd8",
    "_uuid": "40e799459e9f124945e808588cdf1caf0dad7b9a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check absence of null values\n",
    "for var in X_train.columns:\n",
    "    if X_test[var].isnull().sum()>0:\n",
    "        print(var, X_test[var].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36043b40-c354-4522-aeff-6da2669d6973",
    "_uuid": "d3c05acdac6893988ece948e4f5e10da002e475b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check absence of null values\n",
    "submission_vars = []\n",
    "for var in X_train.columns:\n",
    "    if var!='SalePrice' and submission[var].isnull().sum()>0:\n",
    "        print(var, submission[var].isnull().sum())\n",
    "        submission_vars.append(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "05b40198-b00a-4b64-85d4-15fa78963d49",
    "_uuid": "0e8255d3e977429ddfba68ab38e7499119fc2e4d"
   },
   "source": [
    "This is something important. There are variables in the submission dataset that contain null values (missing data), where in the training set they did not.  This needs to be taken into consideration at the time of making predictions, or deploying models in business scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bdea5752-55fe-49d1-b506-1bae7e16e235",
    "_uuid": "253956d1b7b7aa9a199d0aa40dc6cbc9d593e2ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  I will replace NAN by the median \n",
    "for var in submission_vars:\n",
    "    submission[var].fillna(X_train[var].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2e33cff-2956-4bc5-8203-4843e9f6f78d",
    "_uuid": "21fb93f19da74c5b489a11412af69d4587f036ea"
   },
   "source": [
    "### Transformation of Numerical variables \n",
    "\n",
    "As most variables were skewed, I will transform them with the box cox transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dfb3c0dd-fbbe-4b06-808d-ff56b2ffc05e",
    "_uuid": "cb2368ae9d40bb8a2ff4484133524659079c7b91",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boxcox_transformation(var):\n",
    "    X_train[var], param = stats.boxcox(X_train[var]+1) \n",
    "    X_test[var], param = stats.boxcox(X_test[var]+1) \n",
    "    submission[var], param = stats.boxcox(submission[var]+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92bff9b4-2352-469e-b52e-392fbbe15bd5",
    "_uuid": "baab990e2f877ad9fc4c278c675a81a3d75fbe9c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in continuous:\n",
    "    boxcox_transformation(var)\n",
    "    \n",
    "X_train[continuous].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0775aeaa-d371-403a-924c-def7c72a7220",
    "_uuid": "921874b6729e112b5677cb2d9131cacde366560d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's  check if the transformation created infinite values\n",
    "for var in continuous:\n",
    "    if np.isinf(X_train[var]).sum()>1:\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74745124-22fa-4e3d-a218-399d5d00f45d",
    "_uuid": "d01db1210a9ca8e08af1dd1d6131b59f0ff28d0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in continuous:\n",
    "    if np.isinf(X_test[var]).sum()>1:\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36d85be0-1b35-48ca-b0cf-9423a179b3b0",
    "_uuid": "9fd6c28f272bb02442f9077d3b28d804075733ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in continuous:\n",
    "    if np.isinf(submission[var]).sum()>1:\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3fcfd069-56c9-48db-beb1-8622374ed98a",
    "_uuid": "de741eb26ad78bb68864e49de7eaa208e5e06d36",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check absence of null values(there should be none)\n",
    "for var in X_train.columns:\n",
    "    if X_test[var].isnull().sum()>0:\n",
    "        print(var, X_test[var].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e67ff89-cdb5-4396-8b8c-d827bbc65327",
    "_uuid": "53df59b9561056cc6b8e6581c15a3f8553ad1134",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make boxplots to visualise outliers in the continuous variables\n",
    "# and histograms to get an idea of the distribution\n",
    "# hopefully the transformation yielded variables more \"Gaussian\"looking\n",
    "\n",
    "\n",
    "for var in continuous:\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fig = sns.boxplot(y=X_train[var])\n",
    "    fig.set_title('')\n",
    "    fig.set_ylabel(var)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    fig = sns.distplot(X_train[var].dropna())#.hist(bins=20)\n",
    "    fig.set_ylabel('Number of passengers')\n",
    "    fig.set_xlabel(var)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62d3afa4-1882-47ff-9cec-a98d68fc2f53",
    "_uuid": "28632a2a2d229cb12d069389b3f5a0b8337ab2b2"
   },
   "source": [
    "The boxcox transformation worked for some of the variables, and of course it did not for some others. Those for example where only one value was predominant could not be shaped into a Gaussian looking distribution.\n",
    "\n",
    "Also, notice that there are still outliers in several of the variables. Ideally, we would like to remove them somehow.\n",
    "\n",
    "### Normalisation of the target variable: SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7755317-fc7b-4971-a38c-02661f60880d",
    "_uuid": "a350c2c09c3e34293b3fa6a495c61d0b43809831",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = 'SalePrice'\n",
    "y_train = np.log(y_train) \n",
    "y_test = np.log(y_test) \n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "fig = sns.boxplot(y=y_train)\n",
    "fig.set_title('')\n",
    "fig.set_ylabel(var)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "fig = sns.distplot(y_train)#.hist(bins=20)\n",
    "fig.set_ylabel('Number of passengers')\n",
    "fig.set_xlabel(var)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1ef400e-0fe6-4c2d-bce2-fa0c20d33d5b",
    "_uuid": "15a0063795af537ade335a6148c16ca534e1c444"
   },
   "source": [
    "The transformation of the Sale Variable worked quite well. It shows not a more Gaussian looking shape.\n",
    "\n",
    "### Engineering rare labels in categorical and discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80821c05-8b30-400c-a04e-ca1a9acaef84",
    "_uuid": "c50f6eaa6dc7f33a800f5694001bb29a2dc672e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rare_imputation(variable):\n",
    "    # find frequent labels / discrete numbers\n",
    "    temp = X_train.groupby([variable])[variable].count()/np.float(len(X_train))\n",
    "    frequent_cat = [x for x in temp.loc[temp>0.03].index.values]\n",
    "    \n",
    "    X_train[variable] = np.where(X_train[variable].isin(frequent_cat), X_train[variable], 'Rare')\n",
    "    X_test[variable] = np.where(X_test[variable].isin(frequent_cat), X_test[variable], 'Rare')\n",
    "    submission[variable] = np.where(submission[variable].isin(frequent_cat), submission[variable], 'Rare')\n",
    "    \n",
    "# find unfrequent labels in categorical variables\n",
    "for var in categorical:\n",
    "    rare_imputation(var)\n",
    "    \n",
    "for var in ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars']:\n",
    "    submission[var] = submission[var].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0396bc6e-896e-420b-8046-80850834ff7b",
    "_uuid": "2bab51a67d3805900fa6c5856613c35bb469a28b"
   },
   "source": [
    "\n",
    "### Encode categorical variables\n",
    "\n",
    "I will order the labels according to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9da37dbf-4d96-45b1-8761-4d83a34e9e50",
    "_uuid": "5ebe610c87d2b7b4ba1341dabfdc4df92334cf4a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_categorical_variables(var, target):\n",
    "        # make label to house price dictionary\n",
    "        ordered_labels = X_train.groupby([var])[target].mean().sort_values().index\n",
    "        ordinal_label = {k:i for i, k in enumerate(ordered_labels, 0)} \n",
    "        \n",
    "        # encode variables\n",
    "        X_train[var] = X_train[var].map(ordinal_label)\n",
    "        X_test[var] = X_test[var].map(ordinal_label)\n",
    "        submission[var] = submission[var].map(ordinal_label)\n",
    "\n",
    "# encode labels in categorical vars\n",
    "for var in categorical:\n",
    "    encode_categorical_variables(var, 'SalePrice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a71ae969-5841-4086-b7d8-4b57d970fb33",
    "_uuid": "cd15032dbdf589bf5846687df23b7e972bb19382",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in X_train.columns:\n",
    "    if var!='SalePrice' and submission[var].isnull().sum()>0:\n",
    "        print(var, submission[var].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01e1a469-31e6-4a74-bb21-826a212108bc",
    "_uuid": "f2294eebd285ce14c41b75ca91f40d9bf70dad13",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's inspect the dataset\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e0f5d26-b145-4a9f-a473-11f4220d78b5",
    "_uuid": "d4d26f518ed62082cdc32ab12f1a02aef1a360d1"
   },
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "132b5349-faa4-411d-8a77-2790f1b47a38",
    "_uuid": "952cc5ec9069150f6f687c396b3968c66a5343d0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_vars = [var for var in X_train.columns if var not in ['Id', 'SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38394713-d6d8-41db-abc4-47bb46bdceef",
    "_uuid": "2d56086eecc9f4daf39679072fe380799cda7038",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit scaler\n",
    "scaler = StandardScaler() # create an instance\n",
    "scaler.fit(X_train[training_vars]) #  fit  the scaler to the train set for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e08f058-830b-4a9c-b0b2-515ded429145",
    "_uuid": "45658863e2b2a8cf77cd4634f4dfe4a28117f5e7"
   },
   "source": [
    "The scaler is now ready, we can use it in a machine learning algorithm when required. See below.\n",
    "\n",
    "### Machine Learning algorithm building\n",
    "\n",
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c79674b-4ec6-4799-816e-5fb3cfdec4f9",
    "_uuid": "c49e310e92a5940b298f21eab725aca7b7fe374d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "eval_set = [(X_test[training_vars], y_test)]\n",
    "xgb_model.fit(X_train[training_vars], y_train, eval_set=eval_set, verbose=False)\n",
    "\n",
    "pred = xgb_model.predict(X_train[training_vars])\n",
    "print('xgb train mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "pred = xgb_model.predict(X_test[training_vars])\n",
    "print('xgb test mse: {}'.format(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "339cd514-9ebe-40e9-bb76-d8cb34f6fab3",
    "_uuid": "9ef569bb5aaa4d8cf1066c6a954b70f94b8128cc"
   },
   "source": [
    "#### Support vector classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5814490-714b-4fa5-a969-ffd52ed59247",
    "_uuid": "8cbf7dcd47961b22f98cf4e89f57fb394094aae4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVR_model = SVR()\n",
    "SVR_model.fit(scaler.transform(X_train[training_vars]), y_train)\n",
    "\n",
    "pred = SVR_model.predict(scaler.transform(X_train[training_vars]))\n",
    "print('SVR train mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "pred = SVR_model.predict(scaler.transform(X_test[training_vars]))\n",
    "print('SVR test mse: {}'.format(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "58fa47b6-74ed-4649-9235-bbd162269d09",
    "_uuid": "ffb236c8f6d215efafd4cf80066630ffa4981f5f"
   },
   "source": [
    "#### Regularised linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4bcaa949-686e-46ba-9f34-fa02a35791e3",
    "_uuid": "b6ae81b951cf5b6d2effe70af862fcd1bf3f41a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_model = Lasso(random_state=2909)\n",
    "lin_model.fit(scaler.transform(X_train[training_vars]), y_train)\n",
    "\n",
    "pred = lin_model.predict(scaler.transform(X_train[training_vars]))\n",
    "print('linear train mse: {}'.format(mean_squared_error(y_train, pred)))\n",
    "pred = lin_model.predict(scaler.transform(X_test[training_vars]))\n",
    "print('linear test mse: {}'.format(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51350be4-3d18-48d1-899b-f95cf35fe4f3",
    "_uuid": "92d8cb4621d461887c476a78cd886f14aee51e80"
   },
   "source": [
    "### Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1b9549cb-5e9d-404a-94a1-42c0f8c45303",
    "_uuid": "23b43466137cdb65b13fe9c0d262b07cee40a1c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_ls = []\n",
    "pred_ls.append(pd.Series(xgb_model.predict(submission[training_vars])))\n",
    "\n",
    "pred = SVR_model.predict(scaler.transform(submission[training_vars]))\n",
    "pred_ls.append(pd.Series(pred))\n",
    "\n",
    "pred = lin_model.predict(scaler.transform(submission[training_vars]))\n",
    "pred_ls.append(pd.Series(pred))\n",
    "\n",
    "final_pred = np.exp(pd.concat(pred_ls, axis=1).mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3c1c5423-69d6-4666-9dd8-3b283d198aa3",
    "_uuid": "ba20a4393a5f08280526301ad3d36360539d0a6f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.concat([submission.Id, final_pred], axis=1)\n",
    "temp.columns = ['Id', 'SalePrice']\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "033b91f5-ccd8-4cf1-96ed-1fed901112fd",
    "_uuid": "d2fb10b0fb714627f4d68513fe31c332406d6bcc"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "This solution is not one of the best ranking possible solutions. There is a lot more that can be done to try and improve the quality of the variables before using them in machine learning models. For example, instead of making box plot transformation for all of them, doing it just on those variables that benefit from it, and applying other techniques on the remaining ones, like for example discretisation. Discretisation also takes care of outliers, which do affect the performance of linear models. \n",
    "\n",
    "Variable selection is also an essencial step in the machine learning pipeline, which I have not covered in this notebook.\n",
    "\n",
    "I hope you get a flavour of how to approach (or at least how I would approach) a machine learning problem and that you enjoyed the notebook.\n",
    "\n",
    "Thanks for reading!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df9b0602-2255-4e3a-a2ba-40dafa7acc4d",
    "_uuid": "f3a82fdd24680c8b5b9ec51b654f56b209efc6aa",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
